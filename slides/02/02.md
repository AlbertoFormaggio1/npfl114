title: NPFL114, Lecture 2
class: title, langtech, cc-by-nc-sa
# Training Neural Networks

## Milan Straka

### March 08, 2021

---
section: ML Basics
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
The _bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated.

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

~~~
If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $‚Åáx_1,
\ldots, ‚Åáx_n$ idenpendent and identically distributed random variables, we might
estimate mean and variance as
$$ŒºÃÇ = \frac{1}{n} ‚àë\nolimits_i x_i,~~~œÉÃÇ^2 = \frac{1}{n} ‚àë\nolimits_i (x_i - ŒºÃÇ)^2.$$
~~~
Such an estimate is biased, because $ùîº[œÉÃÇ^2] = (1 - \frac{1}{n})œÉ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance ‚Äì in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.svgz)

---
# Machine Learning Basics

We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.svgz)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than others.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||‚ÜíŒ∏||^2$).

![w=70%,h=center](regularization.svgz)

---
# Machine Learning Basics

**Hyperparameters** are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](double_descent.svgz)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent.svgz)

---
# Why do Neural Networks Generalize so Well

![w=90%,h=center](deep_double_descent_width.svgz)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent_size_epochs.svgz)


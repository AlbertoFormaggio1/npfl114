title: NPFL114, Lecture 12
class: title, langtech, cc-by-nc-sa

# Transformer, External Memory Networks

## Milan Straka

### May 20, 2019

---
section: NASNet
# Neural Architecture Search (NASNet) – 2017

- Using REINFORCE with baseline, we can design neural network architectures.

~~~
- We fix the overall architecture and design only Normal and Reduction cells.
![w=27%,h=center](nasnet_overall.pdf)

---
# Neural Architecture Search (NASNet) – 2017

- Every block is designed by a RNN controller generating individual operations.

![w=100%](nasnet_rnn_controller.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=80%,h=center](nasnet_blocks.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=100%,v=middle](nasnet_performance.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=100%,v=middle](../01/nas_net.pdf)

---
section: Transformer
# Attention is All You Need

For some sequence processing tasks, _sequential_ processing of its elements
might be too restricting. Instead, we may want to combine sequence elements
independently on their distance.

---
# Attention is All You Need

![w=33%,h=center](transformer.pdf)

---
# Attention is All You Need

The attention module for a queries $⇉Q$, keys $⇉K$ and values $⇉V$ is defined as:

$$\textrm{Attention}(⇉Q, ⇉K, ⇉V) = \softmax\left(\frac{⇉Q ⇉K^\top}{\sqrt{d_k}}\right)⇉V.$$

The queries, keys and values are computed from current word representations $⇉W$
using a linear transformation as
$$\begin{aligned}
  ⇉Q &= ⇉V_Q ⋅ ⇉W \\
  ⇉K &= ⇉V_K ⋅ ⇉W \\
  ⇉V &= ⇉V_V ⋅ ⇉W \\
\end{aligned}$$

---
# Attention is All You Need

Multihead attention is used in practice. Instead of using one huge attention, we
split queries, keys and values to several groups (similar to how ResNeXt works),
compute the attention in each of the groups separately, and then concatenate the
results.

![w=75%,h=center](transformer_multihead.pdf)

---
# Attention is All You Need

## Positional Embeddings

We need to encode positional information (which was implicit in RNNs).

~~~
- Learned embeddings for every position.

~~~
- Sinusoids of different frequencies:
  $$\begin{aligned}
    \textrm{PE}_{(\textit{pos}, 2i)} & = \sin\left(\textit{pos} / 10000^{2i/d}\right) \\
    \textrm{PE}_{(\textit{pos}, 2i + 1)} & = \cos\left(\textit{pos} / 10000^{2i/d}\right)
  \end{aligned}$$

  This choice of functions should allow the model to attend to relative
  positions, since for any fixed $k$, $\textrm{PE}_{\textit{pos} + k}$ is
  a linear function of $\textrm{PE}_\textit{pos}$.

---
# Attention is All You Need

Positional embeddings for 20 words of dimension 512, lighter colors representing
values closer to 1 and darker colors representing values closer to -1.
![w=60%,h=center](transformer_positional_embeddings.png)

---
# Attention is All You Need

## Regularization

The network is regularized by:
- dropout of input embeddings,
~~~
- dropout of each sub-layer, just before before it is added to the residual
  connection (and then normalized),
~~~
- label smoothing.

~~~
Default dropout rate and also label smoothing weight is 0.1.

~~~
## Parallel Execution
Training can be performed in parallel because of the _masked attention_ – the
softmax weights of the self-attention are zeroed out not to allow attending
words later in the sequence.

~~~
However, inference is still sequential (and no substantial improvements have
been achieved on parallel inference similar to WaveNet).

---
# Why Attention

![w=100%,v=middle](transformer_attentions.pdf)

---
# Transformers Results

![w=100%,v=middle](transformer_results.pdf)

---
# Transformers Results

![w=78%,h=center](transformer_ablations.pdf)
